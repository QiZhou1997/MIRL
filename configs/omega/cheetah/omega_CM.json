{
    "constant": {
        "env_name": "HalfCheetah-v2",
        "dataset_name": "halfcheetah-medium-v2",
        "save_model_dir": "~/saved_model/halfcheetah-medium-v2",
        "target_entropy": -3,
        "depth_schedule": 10,
        "penalty_coef": 0.2
    },
    "experiment": {
        "tag": "omega",
        "use_gpu": true,
        "seed": null,
        "base_log_dir": "~/data/mirl/omega/cheetah",
        "log_level": "WARNING",
        "repeat": 1
    },
    "algorithm": {
        "sac": {
            "class": "OfflineDynaAlgorithm",
            "kwargs": {
                "num_epochs": 1000,
                "batch_size": 256,
                "num_eval_steps": 10000,
                "num_train_loops_per_epoch": 1000,
                "num_trains_per_train_loop": 1,
                "record_video_freq": -1,
                "silent": false,
                "real_data_ratio": 0.5,
                "imagine_freq": 250,
                "separate_batch": true
            }
        }
    },
    "environment": {
        "eval_env": {
            "class": "ContinuousVectorEnv",
            "kwargs": {
                "env_name": "$env_name",
                "n_env": 10,
                "reward_scale": 1
            }
        }
    },
    "policy": {
        "policy": {
            "class": "gaussian_policy",
            "kwargs": {
                "env": "$eval_env",
                "hidden_layers": [
                    256,
                    256,
                    256
                ],
                "activation": "relu"
            }
        }
    },
    "value": {
        "qf": {
            "class": "OmegaQValue",
            "kwargs": {
                "env": "$eval_env",
                "hidden_layers": [
                    256,
                    256,
                    256
                ],
                "activation": "relu"
            }
        },
        "qf_target": {
            "class": "OmegaQValue",
            "kwargs": {
                "env": "$eval_env",
                "hidden_layers": [
                    256,
                    256,
                    256
                ],
                "activation": "relu"
            }
        }
    },
    "component": {
        "model": {
            "class": "PEModel",
            "kwargs": {
                "env": "$eval_env",
                "known": [
                    "done"
                ],
                "ensemble_size": 20,
                "elite_number": 15,
                "hidden_layers": [
                    200,
                    200,
                    200,
                    200
                ],
                "activation": "swish",
                "connection": "simple",
                "reward_coef": 1,
                "bound_reg_coef": 2e-2,
                "weight_decay": [
                    2.5e-5,
                    5e-5,
                    7.5e-5,
                    7.5e-5,
                    1e-4
                ]
            }
        },
        "model_trainer": {
            "class": "PEModelTrainer",
            "kwargs": {
                "env": "$eval_env",
                "model": "$model",
                "lr": 1e-3,
                "init_model_train_step": 1000,
                "report_freq": 500,
                "max_not_improve": -1,
                "ignore_terminal_state": true,
                "save_dir": null,
                "load_dir": "$save_model_dir"
            }
        },
        "model_collector": {
            "class": "COMPOCollector",
            "kwargs": {
                "model": "$model",
                "policy": "$policy",
                "pool": "$pool", 
                "imagined_data_pool": "$imagined_data_pool",
                "depth_schedule": "$depth_schedule",
                "random_sample": false
            }
        }
    },
    "agent": {
        "agent": {
            "class": "OmegaAgent",
            "kwargs": {
                "env": "$eval_env",
                "policy": "$policy",
                "qf": "$qf",
                "qf_target": "$qf_target",
                "model": "$model",
                "policy_lr": 1e-4,
                "qf_lr": 3e-4,
                "soft_target_tau": 5e-3,
                "use_automatic_entropy_tuning": true,
                "alpha_if_not_automatic": 0,
                "bc_steps": 40000,
                "target_entropy": "$target_entropy",
                "penalty_coef": "$penalty_coef"
            }
        }
    },
    "pool": {
        "pool": {
            "class": "OfflinePool",
            "kwargs": {
                "dataset_name": "$dataset_name",
                "pool_class": "ExtraFieldPool",
                "compute_mean_std": true
            }
        },
        "imagined_data_pool": {
            "class": "simple_pool",
            "kwargs": {
                "env": "$eval_env",
                "max_size": 1e6
            }
        }
    },
    "collector": {
        "eval_collector": {
            "class": "simple_collector",
            "kwargs": {
                "env": "$eval_env",
                "agent": "$agent",
                "memory_size": 10
            }
        }
    }
}